{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset134 PingFangSC-Regular;\f2\fnil\fcharset0 LucidaGrande;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh14820\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Cyberbullying Detection Project\
\
Overview\
(\'85)\
\
Project Structure\
\
cyberbullying/\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf0 \'a9\'c0
\f0 \uc0\u9472 \u9472  data/\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  raw_data/            		# Original dataset (unmodified)\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  interim/             		# Train/val/test splits\
\uc0\u9474    \u9492 \u9472 \u9472  processed_data/      		# Cleaned, labeled, preprocessed data + label mapping\

\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  notebooks/\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_cleaning.ipynb  		# Exploratory cleaning\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_understanding.ipynb  	# Dataset statistics\
\uc0\u9474    \u9492 \u9472 \u9472  workflow.ipynb       		# Main pipeline execution\

\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  scripts/\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  config.py            		# Project-wide constants\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_loader.py       		# Dataset loading\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_understanding.py	# Exploratory analysis tools\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  text_preprocessing.py		# Full/soft text cleaning functions\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  language_detection.py	# English filtering using langdetect\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_splitting.py    		# Train/val/test splitting with stratification\
\uc0\u9474    
\f1 \'a9\'c0
\f0 \uc0\u9472 \u9472  data_saver.py        		# Save DataFrame or splits\
\uc0\u9474    \u9492 \u9472 \u9472  data_builder.py      		# Adds labels and saves label map\
\uc0\u9492 \u9472 \u9472  outputs/                 			# Model outputs and temporary results\
```\
\
Workflow (notebook: `workflow.ipynb`)\
\
Step 1: Project Setup\
- Set base path\
- Import modules and config\
\
Step 2: Load Dataset\
- Load raw CSV using `DataLoader`\
\
Step 3: Data Understanding\
- Show class distribution\
- Check for imbalance\
- Explore tweet length, hashtags, emoji frequency\
\
Step 4: Language Filtering\
- Filter only English tweets using `langdetect`\
\
Step 5: Text Preprocessing\
- `tweet_soft`: For Transformers 
\f2 \uc0\u8594 
\f0  removes mentions, links, hashtags only\
- `tweet_full`: For traditional ML 
\f2 \uc0\u8594 
\f0  removes stopwords, stemming, lemmatization\
\
Step 6: Label Encoding\
- Add:\
  - `is_cyberbullying` 
\f2 \uc0\u8594 
\f0  binary label (0/1)\
  - `cyberbullying_label` 
\f2 \uc0\u8594 
\f0  multiclass label (int)\
- Save label map as `label_mapping.json`\
- Save full preprocessed dataset\
\
Step 7: Data Splitting\
- Stratified split into train/val/test (90% dev, 10% test 
\f2 \uc0\u8594 
\f0  then 81/19 split of dev)\
\
Step 8: Save Splits\
- Save `train.csv`, `val.csv`, `test.csv` into `data/interim/`\
\
Preprocessing Logic\
\
 `tweet_soft`\
- Keeps linguistic richness for Transformers\
- Removes: `@mentions`, URLs, `#` symbols, HTML tags, excessive whitespace\
\
`tweet_full`\
- For TF-IDF, BoW, RNNs\
- Expands contractions\
- Normalizes repeated characters\
- Lowercases\
- Removes stopwords\
- Lemmatizes\
- Stems\
- Removes punctuation and cleans spacing\
\
Labeling\
- Binary: `0` if "not_cyberbullying", else `1`\
- Multiclass: encoded using `LabelEncoder`, mapping saved as JSON\
\
Outputs\
- `dataset_preprocessed.csv`: full preprocessed dataset with all versions and labels\
- `train.csv`, `val.csv`, `test.csv`: stratified splits ready for model training\
- `label_mapping.json`: class-to-index mapping for model output interpretation\
\
Notes\
- Transformers should use `tweet_soft`\
- Traditional models (Logistic Regression, Naive Bayes, LSTM) should use `tweet_full`\
- Preprocessing is cleanly modularized in `TextPreprocessor`\
- You can run everything from `workflow.ipynb` step-by-step\
\
Dependencies\
Install via:\
```bash\
pip install -r requirements.txt\
python -m spacy download en_core_web_sm\
```\
Required packages include:\
- pandas\
- scikit-learn\
- spacy\
- nltk\
- contractions\
- langdetect\
- transformers (if using BERT-based models)\
\
\
\
}